{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gptchem.data import get_hea_phase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_hea_phase_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"composition\"] = data[\"Alloy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"composition\", \"phase_binary_encoded\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/utils/data.py:506: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  self.params = pd.read_csv(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/utils/data.py:555: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_params = self.params.append(new_data, sort=True, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from automatminer import MatPipe\n",
    "\n",
    "pipe = MatPipe.from_preset(\"express\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(\n",
    "    data, train_size=10, random_state=42, stratify=data[\"phase_binary_encoded\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:45 INFO     Problem type is: classification\n",
      "2023-02-05 11:10:45 INFO     Fitting MatPipe pipeline to data.\n",
      "2023-02-05 11:10:45 INFO     AutoFeaturizer: Starting fitting.\n",
      "2023-02-05 11:10:45 INFO     AutoFeaturizer: Compositions detected as strings. Attempting conversion to Composition objects...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c87d9b1b0443c2b9ec6d6cd0240bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StrToComposition:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:46 INFO     AutoFeaturizer: Guessing oxidation states of compositions, as they were not present in input.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04537c6d1ab491db0ac2b97cb26ee32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CompositionToOxidComposition:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:48 INFO     AutoFeaturizer: Featurizer type structure not in the dataframe to be fitted. Skipping...\n",
      "2023-02-05 11:10:48 INFO     AutoFeaturizer: Featurizer type bandstructure not in the dataframe to be fitted. Skipping...\n",
      "2023-02-05 11:10:48 INFO     AutoFeaturizer: Featurizer type dos not in the dataframe to be fitted. Skipping...\n",
      "2023-02-05 11:10:48 INFO     AutoFeaturizer: Finished fitting.\n",
      "2023-02-05 11:10:48 INFO     AutoFeaturizer: Starting transforming.\n",
      "2023-02-05 11:10:48 INFO     AutoFeaturizer: Featurizing with ElementProperty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2913fbdf64724e9bae4356dc4a23c80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElementProperty:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:49 INFO     AutoFeaturizer: Featurizing with OxidationStates.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c57dd30d27e47aaa376464bd4313d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "OxidationStates:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:50 INFO     AutoFeaturizer: Featurizing with ElectronAffinity.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe634992a04b43f6bbac1167dd8d21e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElectronAffinity:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:51 INFO     AutoFeaturizer: Featurizing with IonProperty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2124ce15185469aae9a17099d86baf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IonProperty:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:52 INFO     AutoFeaturizer: Featurizing with YangSolidSolution.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a850633f8044247aa8ad1d48c39ebcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "YangSolidSolution:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:53 INFO     AutoFeaturizer: Featurizing with Miedema.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514d5db051b34278986f29f259abbda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Miedema:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:55 INFO     AutoFeaturizer: Featurizer type structure not in the dataframe. Skipping...\n",
      "2023-02-05 11:10:55 INFO     AutoFeaturizer: Featurizer type bandstructure not in the dataframe. Skipping...\n",
      "2023-02-05 11:10:55 INFO     AutoFeaturizer: Featurizer type dos not in the dataframe. Skipping...\n",
      "2023-02-05 11:10:55 INFO     AutoFeaturizer: Finished transforming.\n",
      "2023-02-05 11:10:55 INFO     DataCleaner: Starting fitting.\n",
      "2023-02-05 11:10:55 INFO     DataCleaner: Cleaning with respect to samples with sample na_method 'drop'\n",
      "2023-02-05 11:10:55 INFO     DataCleaner: Replacing infinite values with nan for easier screening.\n",
      "2023-02-05 11:10:55 INFO     DataCleaner: Before handling na: 10 samples, 146 features\n",
      "2023-02-05 11:10:55 INFO     DataCleaner: 0 samples did not have target values. They were dropped.\n",
      "2023-02-05 11:10:55 INFO     DataCleaner: Handling feature na by max na threshold of 0.01 with method 'drop'.\n",
      "2023-02-05 11:10:55 INFO     DataCleaner: These 8 features were removed as they had more than 1.0% missing values: {'avg ionic char', 'minimum oxidation state', 'max ionic char', 'range oxidation state', 'compound possible', 'std_dev oxidation state', 'maximum oxidation state', 'avg anion electron affinity'}\n",
      "2023-02-05 11:10:55 INFO     DataCleaner: After handling na: 10 samples, 138 features\n",
      "2023-02-05 11:10:55 INFO     DataCleaner: Finished fitting.\n",
      "2023-02-05 11:10:55 INFO     FeatureReducer: Starting fitting.\n",
      "2023-02-05 11:10:55 INFO     FeatureReducer: 65 features removed due to cross correlation more than 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:704: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:10:57 INFO     TreeFeatureReducer: Finished tree-based feature reduction of 72 initial features to 38\n",
      "2023-02-05 11:10:57 INFO     FeatureReducer: Finished fitting.\n",
      "2023-02-05 11:10:57 INFO     FeatureReducer: Starting transforming.\n",
      "2023-02-05 11:10:57 INFO     FeatureReducer: Finished transforming.\n",
      "2023-02-05 11:10:57 INFO     TPOTAdaptor: Starting fitting.\n",
      "29 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b80c90bbc7c4cf18313de857471c269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-3\t0.8\tLinearSVC(MaxAbsScaler(SelectPercentile(input_matrix, SelectPercentile__percentile=66)), LinearSVC__C=15.0, LinearSVC__dual=True, LinearSVC__loss=hinge, LinearSVC__penalty=l2, LinearSVC__tol=1e-05)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-3\t0.8\tLinearSVC(MaxAbsScaler(SelectPercentile(input_matrix, SelectPercentile__percentile=66)), LinearSVC__C=15.0, LinearSVC__dual=True, LinearSVC__loss=hinge, LinearSVC__penalty=l2, LinearSVC__tol=1e-05)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _mate_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as metric. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as metric. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by FastICA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as metric. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tGaussianNB(PolynomialFeatures(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.6500000000000001, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "2023-02-05 11:15:28 INFO     TPOTAdaptor: Finished fitting.\n",
      "2023-02-05 11:15:28 INFO     MatPipe successfully fit.\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(train, \"phase_binary_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:15:29 INFO     Beginning MatPipe prediction using fitted pipeline.\n",
      "2023-02-05 11:15:29 INFO     AutoFeaturizer: Starting transforming.\n",
      "2023-02-05 11:15:29 INFO     AutoFeaturizer: Compositions detected as strings. Attempting conversion to Composition objects...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a165e64da954e1b97311d1f3d1321a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StrToComposition:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:15:45 INFO     AutoFeaturizer: Guessing oxidation states of compositions, as they were not present in input.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846cb4f3ba8d42b681d5b346f0056710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CompositionToOxidComposition:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-05 11:16:34 WARNING  AutoFeaturizer: Could not decorate oxidation states due to 'float' object has no attribute 'elements'\n",
      "TO SKIP THESE ERRORS when featurizing specific compounds, set 'ignore_errors=True' when running the batch featurize() operation (e.g., featurize_many(), featurize_dataframe(), etc.).. Some featurizers based on composition oxidstatesmay not work\n",
      "2023-02-05 11:16:34 INFO     AutoFeaturizer: Featurizing with ElementProperty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a4a5e967cf48e8a11d68de0fc55413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElementProperty:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = pipe.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycm import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(\n",
    "    actual_vector=test[\"phase_binary_encoded\"].values,\n",
    "    predict_vector=predictions[\"phase_binary_encoded predicted\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict   0         1         \n",
      "Actual\n",
      "0         272       350       \n",
      "\n",
      "1         1         619       \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Statistics : \n",
      "\n",
      "95% CI                                                            (0.69235,0.74243)\n",
      "ACC Macro                                                         0.71739\n",
      "ARI                                                               0.18859\n",
      "AUNP                                                              0.71784\n",
      "AUNU                                                              0.71784\n",
      "Bangdiwala B                                                      0.59324\n",
      "Bennett S                                                         0.43478\n",
      "CBA                                                               0.53805\n",
      "CSI                                                               0.53541\n",
      "Chi-Squared                                                       343.68828\n",
      "Chi-Squared DF                                                    1\n",
      "Conditional Entropy                                               0.50374\n",
      "Cramer V                                                          0.52604\n",
      "Cross Entropy                                                     1.27337\n",
      "F1 Macro                                                          0.69346\n",
      "F1 Micro                                                          0.71739\n",
      "FNR Macro                                                         0.28216\n",
      "FNR Micro                                                         0.28261\n",
      "FPR Macro                                                         0.28216\n",
      "FPR Micro                                                         0.28261\n",
      "Gwet AC1                                                          0.47571\n",
      "Hamming Loss                                                      0.28261\n",
      "Joint Entropy                                                     1.50374\n",
      "KL Divergence                                                     0.27337\n",
      "Kappa                                                             0.43529\n",
      "Kappa 95% CI                                                      (0.38525,0.48533)\n",
      "Kappa No Prevalence                                               0.43478\n",
      "Kappa Standard Error                                              0.02553\n",
      "Kappa Unbiased                                                    0.38693\n",
      "Krippendorff Alpha                                                0.38717\n",
      "Lambda A                                                          0.43387\n",
      "Lambda B                                                          0.0\n",
      "Mutual Information                                                0.25608\n",
      "NIR                                                               0.50081\n",
      "Overall ACC                                                       0.71739\n",
      "Overall CEN                                                       0.50663\n",
      "Overall J                                                         (1.07474,0.53737)\n",
      "Overall MCC                                                       0.52604\n",
      "Overall MCEN                                                      0.33216\n",
      "Overall RACC                                                      0.49955\n",
      "Overall RACCU                                                     0.53903\n",
      "P-Value                                                           None\n",
      "PPV Macro                                                         0.81757\n",
      "PPV Micro                                                         0.71739\n",
      "Pearson C                                                         0.46556\n",
      "Phi-Squared                                                       0.27672\n",
      "RCI                                                               0.25608\n",
      "RR                                                                621.0\n",
      "Reference Entropy                                                 1.0\n",
      "Response Entropy                                                  0.75981\n",
      "SOA1(Landis & Koch)                                               Moderate\n",
      "SOA2(Fleiss)                                                      Intermediate to Good\n",
      "SOA3(Altman)                                                      Moderate\n",
      "SOA4(Cicchetti)                                                   Fair\n",
      "SOA5(Cramer)                                                      Relatively Strong\n",
      "SOA6(Matthews)                                                    Moderate\n",
      "Scott PI                                                          0.38693\n",
      "Standard Error                                                    0.01278\n",
      "TNR Macro                                                         0.71784\n",
      "TNR Micro                                                         0.71739\n",
      "TPR Macro                                                         0.71784\n",
      "TPR Micro                                                         0.71739\n",
      "Zero-one Loss                                                     351\n",
      "\n",
      "Class Statistics :\n",
      "\n",
      "Classes                                                           0             1             \n",
      "ACC(Accuracy)                                                     0.71739       0.71739       \n",
      "AGF(Adjusted F-score)                                             0.58231       0.84382       \n",
      "AGM(Adjusted geometric mean)                                      0.77318       0.58619       \n",
      "AM(Difference between automatic and manual classification)        -349          349           \n",
      "AUC(Area under the ROC curve)                                     0.71784       0.71784       \n",
      "AUCI(AUC value interpretation)                                    Good          Good          \n",
      "AUPR(Area under the PR curve)                                     0.71682       0.81859       \n",
      "BB(Braun-Blanquet similarity)                                     0.4373        0.6388        \n",
      "BCD(Bray-Curtis dissimilarity)                                    0.1405        0.1405        \n",
      "BM(Informedness or bookmaker informedness)                        0.43569       0.43569       \n",
      "CEN(Confusion entropy)                                            0.54066       0.48746       \n",
      "DOR(Diagnostic odds ratio)                                        481.05143     481.05143     \n",
      "DP(Discriminant power)                                            1.47877       1.47877       \n",
      "DPI(Discriminant power interpretation)                            Limited       Limited       \n",
      "ERR(Error rate)                                                   0.28261       0.28261       \n",
      "F0.5(F0.5 score)                                                  0.79347       0.68839       \n",
      "F1(F1 score - harmonic mean of precision and sensitivity)         0.60782       0.77911       \n",
      "F2(F2 score)                                                      0.49258       0.89736       \n",
      "FDR(False discovery rate)                                         0.00366       0.3612        \n",
      "FN(False negative/miss/type 2 error)                              350           1             \n",
      "FNR(Miss rate or false negative rate)                             0.5627        0.00161       \n",
      "FOR(False omission rate)                                          0.3612        0.00366       \n",
      "FP(False positive/type 1 error/false alarm)                       1             350           \n",
      "FPR(Fall-out or false positive rate)                              0.00161       0.5627        \n",
      "G(G-measure geometric mean of precision and sensitivity)          0.66007       0.79861       \n",
      "GI(Gini index)                                                    0.43569       0.43569       \n",
      "GM(G-mean geometric mean of specificity and sensitivity)          0.66075       0.66075       \n",
      "HD(Hamming distance)                                              351           351           \n",
      "IBA(Index of balanced accuracy)                                   0.19163       0.68156       \n",
      "ICSI(Individual classification success index)                     0.43364       0.63719       \n",
      "IS(Information score)                                             0.99238       0.35577       \n",
      "J(Jaccard index)                                                  0.4366        0.63814       \n",
      "LS(Lift score)                                                    1.98947       1.27967       \n",
      "MCC(Matthews correlation coefficient)                             0.52604       0.52604       \n",
      "MCCI(Matthews correlation coefficient interpretation)             Moderate      Moderate      \n",
      "MCEN(Modified confusion entropy)                                  0.48225       0.54087       \n",
      "MK(Markedness)                                                    0.63514       0.63514       \n",
      "N(Condition negative)                                             620           622           \n",
      "NLR(Negative likelihood ratio)                                    0.56361       0.00369       \n",
      "NLRI(Negative likelihood ratio interpretation)                    Negligible    Good          \n",
      "NPV(Negative predictive value)                                    0.6388        0.99634       \n",
      "OC(Overlap coefficient)                                           0.99634       0.99839       \n",
      "OOC(Otsuka-Ochiai coefficient)                                    0.66007       0.79861       \n",
      "OP(Optimized precision)                                           0.32658       0.32658       \n",
      "P(Condition positive or support)                                  622           620           \n",
      "PLR(Positive likelihood ratio)                                    271.1254      1.77428       \n",
      "PLRI(Positive likelihood ratio interpretation)                    Good          Poor          \n",
      "POP(Population)                                                   1242          1242          \n",
      "PPV(Precision or positive predictive value)                       0.99634       0.6388        \n",
      "PRE(Prevalence)                                                   0.50081       0.49919       \n",
      "Q(Yule Q - coefficient of colligation)                            0.99585       0.99585       \n",
      "QI(Yule Q interpretation)                                         Strong        Strong        \n",
      "RACC(Random accuracy)                                             0.11008       0.38947       \n",
      "RACCU(Random accuracy unbiased)                                   0.12982       0.40921       \n",
      "TN(True negative/correct rejection)                               619           272           \n",
      "TNR(Specificity or true negative rate)                            0.99839       0.4373        \n",
      "TON(Test outcome negative)                                        969           273           \n",
      "TOP(Test outcome positive)                                        273           969           \n",
      "TP(True positive/hit)                                             272           619           \n",
      "TPR(Sensitivity, recall, hit rate, or true positive rate)         0.4373        0.99839       \n",
      "Y(Youden index)                                                   0.43569       0.43569       \n",
      "dInd(Distance index)                                              0.5627        0.5627        \n",
      "sInd(Similarity index)                                            0.60211       0.60211       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f3b9074e5baa1438c27e2ea813f7f53b7516c83bd70840b6d64eae6820ee5df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
